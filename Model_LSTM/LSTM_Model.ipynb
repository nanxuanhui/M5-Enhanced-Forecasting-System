{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33aa589a",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c69d49",
   "metadata": {},
   "source": [
    "This script implements a deep learning model using an LSTM network to forecast daily unit sales,\n",
    "leveraging historical sales data, calendar effects, and engineered features from the M5 Forecasting Challenge.\n",
    "\n",
    "The model pipeline includes:\n",
    "- Data loading from the preprocessed feature file\n",
    "- Label encoding of categorical variables\n",
    "- Feature normalization\n",
    "- Construction of temporal sequences for LSTM input\n",
    "- Model training with early stopping and RMSE evaluation\n",
    "- Visualization of predictions and training history\n",
    "- Output of predictions on a held-out test set for inference or submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724a1fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea128b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "print(\"Loading preprocessed data...\")\n",
    "df = pd.read_pickle(\"/Users/nanxuan/Desktop/M5 Enhanced Forecasting System/Dataset/processed_sales_data.pkl\")\n",
    "df.dropna(subset=[\"sales_lag_7\", \"rolling_mean_7\", \"price_change\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a539b7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding categorical variables...\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical columns\n",
    "print(\"Encoding categorical variables...\")\n",
    "categorical_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705015d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing features...\n"
     ]
    }
   ],
   "source": [
    "# Select features\n",
    "features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
    "            'day_of_week', 'month', 'year', 'is_weekend', 'is_us_holiday',\n",
    "            'sales_lag_7', 'rolling_mean_7', 'price_change']\n",
    "target = 'sales'\n",
    "\n",
    "# Normalize features\n",
    "print(\"Normalizing features...\")\n",
    "normalize_cols = ['sales_lag_7', 'rolling_mean_7', 'price_change', 'day_of_week', 'month', 'year']\n",
    "scaler = MinMaxScaler()\n",
    "df[normalize_cols] = scaler.fit_transform(df[normalize_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d19f5b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grouped sequences...\n"
     ]
    }
   ],
   "source": [
    "# Construct sequences grouped by 'id'\n",
    "def create_grouped_sequences(df, features, target, sequence_length=28, group_col='id', max_seq_per_group=100):\n",
    "    X, y = [], []\n",
    "    for _, group in df.groupby(group_col):\n",
    "        group = group.sort_values('date')\n",
    "        if len(group) >= sequence_length + 1:\n",
    "            n_seq = min(len(group) - sequence_length, max_seq_per_group)\n",
    "            for i in range(n_seq):\n",
    "                seq_x = group[features].iloc[i:i+sequence_length].values\n",
    "                seq_y = group[target].iloc[i+sequence_length]\n",
    "                X.append(seq_x)\n",
    "                y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "print(\"Creating grouped sequences...\")\n",
    "X_all, y_all = create_grouped_sequences(df, features, target, sequence_length=28, group_col='id', max_seq_per_group=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cbd988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data...\n"
     ]
    }
   ],
   "source": [
    "# Train/val split\n",
    "print(\"Splitting data...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_all, y_all, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d99fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_t = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d87dd511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    }
   ],
   "source": [
    "# Define LSTM model\n",
    "class SalesLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(SalesLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Take last output\n",
    "        return out.squeeze()\n",
    "\n",
    "print(\"Initializing model...\")\n",
    "model = SalesLSTM(input_size=X_train.shape[2], hidden_size=64).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc35be74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model...\n",
      "Epoch 1/20, Train Loss: 16.0464, Val Loss: 15.9263\n",
      "Epoch 2/20, Train Loss: 15.7666, Val Loss: 15.0843\n",
      "Epoch 3/20, Train Loss: 14.4583, Val Loss: 14.2456\n",
      "Epoch 4/20, Train Loss: 14.6579, Val Loss: 15.5475\n",
      "Epoch 5/20, Train Loss: 14.4603, Val Loss: 15.6237\n",
      "Epoch 6/20, Train Loss: 15.4990, Val Loss: 15.2675\n",
      "Epoch 7/20, Train Loss: 13.6773, Val Loss: 13.2939\n",
      "Epoch 8/20, Train Loss: 13.6007, Val Loss: 15.3893\n",
      "Epoch 9/20, Train Loss: 15.4767, Val Loss: 15.7271\n",
      "Epoch 10/20, Train Loss: 14.3516, Val Loss: 15.4487\n",
      "Epoch 11/20, Train Loss: 14.8717, Val Loss: 15.7589\n",
      "Epoch 12/20, Train Loss: 14.8557, Val Loss: 14.9637\n",
      "Epoch 13/20, Train Loss: 13.6743, Val Loss: 13.9418\n",
      "Epoch 14/20, Train Loss: 13.1349, Val Loss: 12.0455\n",
      "Epoch 15/20, Train Loss: 15.2651, Val Loss: 10.9511\n",
      "Epoch 16/20, Train Loss: 12.8145, Val Loss: 14.0845\n",
      "Epoch 17/20, Train Loss: 13.7819, Val Loss: 16.2911\n",
      "Epoch 18/20, Train Loss: 13.7464, Val Loss: 14.4302\n",
      "Epoch 19/20, Train Loss: 14.3406, Val Loss: 15.6640\n",
      "Epoch 20/20, Train Loss: 14.7815, Val Loss: 14.2774\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"Training LSTM model...\")\n",
    "epochs = 20\n",
    "train_losses, val_losses = [], []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(xb)\n",
    "        loss = criterion(output, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            output = model(xb)\n",
    "            loss = criterion(output, yb)\n",
    "            val_loss += loss.item()\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab76cc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 3.7774\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "preds = model(X_val_t).detach().cpu().numpy()\n",
    "true = y_val_t.detach().cpu().numpy()\n",
    "rmse = np.sqrt(mean_squared_error(true, preds))\n",
    "print(f\"Validation RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1242252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
